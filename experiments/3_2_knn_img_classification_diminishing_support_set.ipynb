{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb==0.3.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS AND IMPORTS\n",
    "import timm\n",
    "import chromadb\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "def make_embeddings(dataloader, model, classes, device):\n",
    "    data_source = \"\" # E.g. \"torch_cifar10\": string added as metadata to DB. Currently not further used, but might be helpful to determine data origin.\n",
    "    apped_str_to_ids = \"\" # string concatinated to each id string in DB. Helpful if multiple datasets should be added to the same DB\n",
    "    d = []\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        images = X\n",
    "        labels = y\n",
    "        images = images.to(device)\n",
    "\n",
    "        output = model.forward_features(images)\n",
    "        output = model.forward_head(output, pre_logits=True)\n",
    "\n",
    "        feature_maps = output\n",
    "\n",
    "\n",
    "        for id, feature_map in enumerate(feature_maps):\n",
    "            d.append(\n",
    "                {\n",
    "                    'document': f\"X[{id}]\", # str(\"X[\" + str(id) + \"]\"), # document is currently irrelevant. Might be used to add raw input data to DB\n",
    "                    'embedding': feature_map.view(-1).cpu().tolist(), # feature_map.flatten().tolist(), # this is important not efficient on GPU\n",
    "                    'metadata': {\"label\": classes[y[id]], \"data_source\": data_source}, # can add any metadata. \"label\" was used for ACC calculation on test data\n",
    "                    'id': f\"id_b{batch}_id{id}{apped_str_to_ids}\",  #str(\"id_b\" + str(batch) + \"_id\" + str(id) + apped_str_to_ids),\n",
    "                    'class_id': y[id].item()\n",
    "                }\n",
    "            )\n",
    "\n",
    "        ### ADD SOME GARBAGE COLLECTION\n",
    "        del X\n",
    "        del y\n",
    "        del images\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "def add_train_test_df_to_chroma_collection(chroma_collection, df):\n",
    "    chroma_collection.upsert(\n",
    "        documents=list(df['document']),\n",
    "        embeddings=list(df['embedding']),\n",
    "        metadatas=list(df['metadata']),\n",
    "        ids=list(df['id'])\n",
    "    )\n",
    "    print(\"Elements in DB after add:\", chroma_collection.count())  # returns the number of items in the collection\n",
    "    #print(collection.peek()) # returns a list of the first 10 items in the collection\n",
    "\n",
    "# PHASE 3 (inference)\n",
    "def eval_collection_against_df(collection_loaded, df_to_test, classes, k = 10, distance_weighted = False):\n",
    "        total_correct = 0\n",
    "        total_instances = 0\n",
    "\n",
    "        for sample in df_to_test.itertuples():\n",
    "            print(\"Testing \" + str(total_instances) + \" out of \" + str(len(df_to_test.index)), end=\"\\r\")\n",
    "            result = collection_loaded.query(\n",
    "                query_embeddings=list(sample.embedding),\n",
    "                n_results = k,\n",
    "                include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "            )\n",
    "\n",
    "            if distance_weighted == False:\n",
    "                classification = knn_majority_vote(result, k)\n",
    "            else:\n",
    "                classification = knn_distance_weighted(result, k)\n",
    "\n",
    "            if (classification == classes[sample.class_id]):\n",
    "                total_correct+=1\n",
    "                #print(\"Correct classification\")\n",
    "            #else:\n",
    "                #print(\"Missclassification: Real: \" + classes[sample.class_id] + \"; Classification:\" + classification)\n",
    "                #print(display_distances_with_labels(result))\n",
    "            total_instances+=1\n",
    "\n",
    "        acc = round(total_correct/total_instances, 3)\n",
    "        #print(\"Total test instances: \" + str(total_instances))\n",
    "        #print(\"Total correct of test instances: \" + str(total_correct))\n",
    "        #print(\"Accuracy: \" +  str(acc))\n",
    "\n",
    "        return acc\n",
    "\n",
    "def knn_majority_vote(data, k):\n",
    "    distances = data['distances'][0]\n",
    "    labels = [metadata['label'] for metadata in data['metadatas'][0]]\n",
    "\n",
    "    # Get the k nearest neighbors based on distances\n",
    "    nearest_neighbors = sorted(range(len(distances)), key=lambda x: distances[x])[:k]\n",
    "\n",
    "    # Extract the corresponding labels for the nearest neighbors\n",
    "    nearest_labels = [labels[i] for i in nearest_neighbors]\n",
    "\n",
    "    # Perform majority vote on the labelss\n",
    "    vote_counts = Counter(nearest_labels)\n",
    "\n",
    "    # Get the label with the maximum vote count\n",
    "    majority_label = vote_counts.most_common(1)[0][0]\n",
    "\n",
    "    return majority_label\n",
    "\n",
    "def knn_distance_weighted(data, k):\n",
    "    distances = data['distances'][0]\n",
    "    labels = [metadata['label'] for metadata in data['metadatas'][0]]\n",
    "\n",
    "    # Get the k nearest neighbors based on distances\n",
    "    nearest_neighbors = sorted(range(len(distances)), key=lambda x: distances[x])[:k]\n",
    "\n",
    "    # Create a dictionary to store label weights\n",
    "    label_weights = {}\n",
    "\n",
    "    # Calculate distance-weighted votes for each label\n",
    "    for i in nearest_neighbors:\n",
    "        if (distances[i] != 0):\n",
    "            weight = 1.0 / distances[i]\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        label = labels[i]\n",
    "        if label in label_weights:\n",
    "            label_weights[label] += weight\n",
    "        else:\n",
    "            label_weights[label] = weight\n",
    "\n",
    "    # Find the label with the maximum weighted vote\n",
    "    majority_label = max(label_weights, key=label_weights.get)\n",
    "\n",
    "    return majority_label\n",
    "\n",
    "# Let's define a custom Dataset class for our data\n",
    "class datasetISIC2018DiseaseClassification(Dataset):\n",
    "    def __init__(self, csv_file, class_list, transform=None, subset = \"train\"):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.class_list = class_list\n",
    "        self.subset = subset\n",
    "        self.classes = [\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    # def __getitem__(self, index):\n",
    "    #     image = Image.open(\"assets/datasets/ISIC2018/Task3-DiseaseClassification/\"+ self.subset +\"/\" + self.df.image[index] + \".jpg\")\n",
    "    #     #print(self.df.label[index])\n",
    "    #     #print(self.class_list[self.df.label[index]])\n",
    "    #     #label = self.class_list[self.df.label[index]]\n",
    "    #     label = self.df.label[index]\n",
    "\n",
    "    #     if self.transform:\n",
    "    #         image = self.transform(image)\n",
    "    #     return image, label\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(\"assets/datasets/ISIC2018/Task3-DiseaseClassification/\" + self.subset + \"/\" + self.df.image[index] + \".jpg\")\n",
    "        \n",
    "        label_row = self.df.iloc[index, 1:]\n",
    "        label = label_row.idxmax()\n",
    "        label_index = self.classes.index(label)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # print(f\"Loaded image: {self.df.image[index]}, Label: {label_index}\")\n",
    "        \n",
    "        return image, label_index\n",
    "\n",
    "# EXPERIMENT SPECIFIC METHODS\n",
    "def copy_collection(collection,chroma_client, name_collection):\n",
    "    #collection.peek() # returns a list of the first 10 items in the collection\n",
    "    print(\"Elements in DB1:\", collection.count())  # returns the number of items in the collection\n",
    "\n",
    "    collection_copied = chroma_client.get_or_create_collection(name=name_collection+\"_copy\",\n",
    "                                                metadata={\"hnsw:space\": \"cosine\"} # set cosine similarity as distance function\n",
    "                                                )\n",
    "\n",
    "    db1_data=collection.get(include=['documents','metadatas','embeddings'])\n",
    "    collection_copied.upsert(\n",
    "        embeddings=db1_data['embeddings'],\n",
    "        metadatas=db1_data['metadatas'],\n",
    "        documents=db1_data['documents'],\n",
    "        ids=db1_data['ids']\n",
    "    )\n",
    "    #collection_copied.peek() # returns a list of the first 10 items in the collection\n",
    "    print(\"Elements in DB2 after copy:\", collection_copied.count())  # returns the number of items in the collection\n",
    "    return collection_copied\n",
    "\n",
    "def copy_collection_delete_most_significant_n_and_eval(id_frequencies, n, collection, chroma_client, name_collection, df_test, classes):\n",
    "    ids_key_elements_to_be_deleted = list()\n",
    "    for label, elements in id_frequencies.items():\n",
    "        #print(\"Label:\", label)\n",
    "        #print(\"Most Common \"+str(n)+\" Elements:\", id_frequencies[label].most_common(n))\n",
    "        for item in id_frequencies[label].most_common(n):\n",
    "            ids_key_elements_to_be_deleted.append(item[0])\n",
    "\n",
    "    collection_copied = copy_collection(collection, chroma_client, name_collection)\n",
    "    collection_copied.delete(ids_key_elements_to_be_deleted)\n",
    "    chroma_client.persist()\n",
    "    print(\"Elements in DB2 after delete:\", collection_copied.count())\n",
    "    return eval_collection_against_df(collection_copied, df_test, classes)\n",
    "\n",
    "def copy_collection_delete_random_n_and_eval(label_arrays, n, collection, chroma_client, name_collection, df_test, classes, ids_random_elements_already_deleted):\n",
    "    np.random.seed(5)  # always set the same seed\n",
    "\n",
    "    ids_random_elements_to_be_deleted = []\n",
    "    \n",
    "    # Flatten the list of all available IDs across all classes\n",
    "    all_ids = [id for ids in label_arrays.values() for id in ids if id not in ids_random_elements_already_deleted]\n",
    "    \n",
    "    if len(all_ids) < n:\n",
    "        raise ValueError(f\"Not enough elements to delete. Needed: {n}, Available: {len(all_ids)}\")\n",
    "    \n",
    "    # Select exactly n unique IDs to delete\n",
    "    selected_elements = np.random.choice(all_ids, n, replace=False)\n",
    "    ids_random_elements_to_be_deleted.extend(selected_elements)\n",
    "\n",
    "    print(\"Random IDs to be deleted: n = \" + str(len(ids_random_elements_to_be_deleted)))\n",
    "    print(ids_random_elements_to_be_deleted)\n",
    "\n",
    "    # Copy the collection\n",
    "    collection_copied = copy_collection(collection, chroma_client, name_collection)\n",
    "\n",
    "    # Delete the selected elements from the copied collection\n",
    "    collection_copied.delete(ids_random_elements_to_be_deleted)\n",
    "    chroma_client.persist()\n",
    "    \n",
    "    print(\"Elements in DB2 after delete:\", collection_copied.count())\n",
    "    \n",
    "    return ids_random_elements_to_be_deleted, eval_collection_against_df(collection_copied, df_test, classes)\n",
    "    \n",
    "def eval_collection_against_df_count_most_significant(collection_loaded, df_to_test, classes, k = 10, distance_weighted = False):\n",
    "    ## Initialize a Counter for ID frequencies\n",
    "    ## Initialize a dictionary of Counters for each label\n",
    "    id_frequencies = {label: Counter() for label in classes}\n",
    "\n",
    "    total_correct = 0\n",
    "    total_instances = 0\n",
    "\n",
    "    for sample in df_to_test.itertuples():\n",
    "        print(\"Testing \" + str(total_instances) + \" out of \" + str(len(df_to_test.index)), end=\"\\r\")\n",
    "        result = collection_loaded.query(\n",
    "            query_embeddings=list(sample.embedding),\n",
    "            n_results = k,\n",
    "            include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        #print(display_distances_with_labels(result))\n",
    "        if distance_weighted == False:\n",
    "            classification = knn_majority_vote(result, k)\n",
    "        else:\n",
    "            classification = knn_distance_weighted(result, k)\n",
    "\n",
    "        if (classification == classes[sample.class_id]):\n",
    "            total_correct+=1\n",
    "            #print(\"Correct classification\")\n",
    "            ## COUNTER\n",
    "            metadatas = result['metadatas'][0]\n",
    "            ids = result['ids'][0]\n",
    "\n",
    "            ## Loop through the elements and only pass those with label \"horse\"\n",
    "            for metadata, id in zip(metadatas, ids):\n",
    "                label = metadata['label']\n",
    "                id_frequencies[label][id] += 1\n",
    "        #else:\n",
    "            #print(\"Missclassification: Real: \" + classes[sample.class_id] + \"; Classification:\" + classification)\n",
    "            #print(display_distances_with_labels(result))\n",
    "        total_instances+=1\n",
    "\n",
    "    acc = round(total_correct/total_instances, 3)\n",
    "    print(\"Total test instances: \" + str(total_instances))\n",
    "    print(\"Total correct of test instances: \" + str(total_correct))\n",
    "    print(\"Accuracy: \" +  str(acc))\n",
    "    return acc, id_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import remove\n",
    "# SETUP PARAMETERS Melanoma diminishing support set (MVF delete)\n",
    "import ast\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" # TODO choose your cpu if no gpu is available\n",
    "print(\"Using device: \" + device)\n",
    "dataset = \"melanoma\" # TODO choose your dataset (ours: \"melanoma\", \"pneumonia\")\n",
    "pretrain = False\n",
    "db_name = \"privacy-aware-image-classification-with-kNN\" # TODO name that allows you to identify db in the future\n",
    "db_collection_name = f\"{db_name}_ex3_2\" # TODO name that allows you to identify your collection in the future\n",
    "db_persistent_directory = \"chroma/databases/\" + db_name # TODO adjust to your own chroma storage path\n",
    "\n",
    "# DATABASE CHROMA\n",
    "chroma_client = chromadb.Client(chromadb.config.Settings(\n",
    "            chroma_db_impl = \"duckdb+parquet\",\n",
    "            persist_directory = db_persistent_directory\n",
    "        ))\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=db_collection_name)\n",
    "print(\"Elements in loaded collection: \" + str(chroma_collection.count())) # should typically be empty\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "# any backbone from https://huggingface.co/timm\n",
    "backbone = \"vit_large_patch14_dinov2.lvd142m\" # TODO choose your timm backbone (ours: \"vit_small_patch14_dinov2.lvd142m\",\"vit_large_patch14_dinov2.lvd142m\",\"vit_base_patch16_clip_224.openai\",\"vit_large_patch14_clip_336.openai\")\n",
    "\n",
    "# MODEL\n",
    "model = timm.create_model(backbone, pretrained=True, num_classes=0).to(device)\n",
    "model = model.eval()\n",
    "\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transform = timm.data.create_transform(**data_config, is_training=False) # get the required transform for the given backbone\n",
    "if dataset == \"melanoma\":\n",
    "        print(\"Melanoma dataset\")\n",
    "        removed_features_per_class = [1,5,10,20,30,40,50,60,70,80,90,100]\n",
    "        # load the csv file\n",
    "        csv_file_path = \"assets/datasets/ISIC2018/Task3-DiseaseClassification/isic_labels_unified_TRAIN.csv\"\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        print(df.head())\n",
    "\n",
    "        # Define and map the class label\n",
    "        class_labels = [\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]\n",
    "        #{'MEL': 0, 'NV': 1, 'BCC': 2, 'AKIEC': 3, 'BKL': 4, 'DF': 5, 'VASC': 6}\n",
    "        # melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowen’s disease, benign keratosis, dermatofibroma, and vascular lesion\n",
    "        class_labels_map = {}\n",
    "        for indx, label in enumerate(class_labels):\n",
    "            class_labels_map[indx] = label\n",
    "\n",
    "        # Lets create an object from our custom dataset class\n",
    "        train_dataset = datasetISIC2018DiseaseClassification(csv_file_path, class_labels, transform, \"train\")\n",
    "\n",
    "        classes = train_dataset.classes\n",
    "        print(f\"classes: {classes} type:{type(classes)}\")\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            print(f\"Batch {batch_idx}: data shape = {data.shape}, target = {target}\")\n",
    "            if batch_idx == 0:  # Print only the first batch for verification\n",
    "                break\n",
    "\n",
    "        # GET TRAIN EMBEDDINGS (support set)\n",
    "        print('Calculate feature maps for support set embeddings...')\n",
    "        df_train = make_embeddings(train_loader, model, classes, device)\n",
    "        df_train.to_csv(\"isic_labels_unified_TRAIN_embeddings.csv\", index=False)\n",
    "        print(df_train.head())\n",
    "        add_train_test_df_to_chroma_collection(chroma_collection,df_train)\n",
    "        chroma_client.persist() # data is only stored in Chroma DB when persist() is called\n",
    "\n",
    "        assert chroma_collection.count() == (len(df_train))\n",
    "\n",
    "        ## SUPPORT SET HAS BEEN ADDED TO DB ##\n",
    "        # EVALUATE ACCURACY MELANOMA\n",
    "        # load the csv file\n",
    "        csv_file_path = \"assets/datasets/ISIC2018/Task3-DiseaseClassification/isic_labels_unified_TEST.csv\"\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Define and map the class label\n",
    "        class_labels = [\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]\n",
    "        #{'MEL': 0, 'NV': 1, 'BCC': 2, 'AKIEC': 3, 'BKL': 4, 'DF': 5, 'VASC': 6}\n",
    "        # melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowen’s disease, benign keratosis, dermatofibroma, and vascular lesion\n",
    "        class_labels_map = {}\n",
    "        for indx, label in enumerate(class_labels):\n",
    "            class_labels_map[indx] = label\n",
    "\n",
    "        # Lets create an object from our custom dataset class\n",
    "        test_dataset = datasetISIC2018DiseaseClassification(csv_file_path, class_labels, transform, \"test\")\n",
    "\n",
    "        classes = test_dataset.classes\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "        df_test = make_embeddings(test_loader, model, classes, device)\n",
    "        df_test.to_csv(\"isic_labels_unified_TEST_embeddings.csv\", index=False)\n",
    "        print(df_test.head())\n",
    "\n",
    "elif dataset == \"pneumonia\":\n",
    "        print(\"Pneumonia dataset\")\n",
    "        removed_features_per_class = [1,5,10,20,50,100,200,300,500,700,900]\n",
    "        # DATASET # TODO choose your support dataset (ours: \"melanoma\", \"pneumonia\")\n",
    "        train_dataset = torchvision.datasets.ImageFolder('assets/datasets/Pneumonia/train', transform=transform)\n",
    "        classes = ['NORMAL', 'PNEUMONIA'] if pretrain else train_dataset.classes\n",
    "        print(f\"classes: {classes} type:{type(classes)}\")\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "        # GET TRAIN EMBEDDINGS (support set)\n",
    "        print('Calculate feature maps for support set embeddings...')\n",
    "        if pretrain:\n",
    "            df_train = pd.read_csv(\"pneumonia_train_embeddings.csv\")\n",
    "            df_train['embedding'] = df_train['embedding'].apply(ast.literal_eval)\n",
    "            df_train['metadata'] = df_train['metadata'].apply(ast.literal_eval)\n",
    "\n",
    "        else:\n",
    "            df_train = make_embeddings(train_loader, model, classes, device)\n",
    "            df_train.to_csv(\"pneumonia_train_embeddings.csv\", index=False)\n",
    "        print(df_train.head())\n",
    "        add_train_test_df_to_chroma_collection(chroma_collection,df_train)\n",
    "        chroma_client.persist() # data is only stored in Chroma DB when persist() is called\n",
    "\n",
    "        assert chroma_collection.count() == (len(df_train))\n",
    "\n",
    "        ## SUPPORT SET HAS BEEN ADDED TO DB ##\n",
    "        # EVALUATE ACCURACY PNEUMONIA\n",
    "        # DATASET # TODO choose your test dataset (ours: \"melanoma\", \"pneumonia\")\n",
    "        test_dataset = torchvision.datasets.ImageFolder('assets/datasets/Pneumonia/test', transform=transform)\n",
    "\n",
    "        classes = ['NORMAL', 'PNEUMONIA'] if pretrain else test_dataset.classes\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "        if pretrain:\n",
    "          df_test = pd.read_csv(\"pneumonia_test_embeddings.csv\")\n",
    "          df_test['embedding'] = df_test['embedding'].apply(ast.literal_eval)\n",
    "          df_test['metadata'] = df_test['metadata'].apply(ast.literal_eval)\n",
    "        else:\n",
    "          df_test = make_embeddings(test_loader, model, classes, device)\n",
    "          df_test.to_csv(\"pneumonia_test_embeddings.csv\", index=False)\n",
    "        \n",
    "        print(df_test.head())\n",
    "else:\n",
    "        print(\"Unknown dataset\")\n",
    "        exit()\n",
    "\n",
    "# Evaluate baseline acc with most significant elements\n",
    "acc_baseline, id_frequencies = eval_collection_against_df_count_most_significant(chroma_collection, df_test,classes)\n",
    "print(backbone + \":Classes \" + str(df_test.class_id.unique()) + \": acc_baseline= \" + str(acc_baseline))\n",
    "\n",
    "results = {}\n",
    "# Stepwise remove i most significant elements\n",
    "for i in removed_features_per_class:\n",
    "    last_acc = copy_collection_delete_most_significant_n_and_eval(id_frequencies, i, chroma_collection, chroma_client, db_collection_name, df_test, classes)\n",
    "    print(backbone + \":Delete \"+str(i)+\" most significant elements from each class. Acc= \" + str(last_acc))\n",
    "    results[str(i)] = last_acc\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP PARAMETERS Melanoma diminishing support set (random delete)\n",
    "import ast\n",
    "device = \"cuda:0\" # TODO choose your ML computing device\n",
    "dataset = \"melanoma\" # TODO choose your dataset (ours: \"melanoma\", \"pneumonia\")\n",
    "pretrain = True\n",
    "db_name = \"privacy-aware-image-classification-with-kNN\" # TODO name that allows you to identify db in the future\n",
    "db_collection_name = db_name + \"_\" + \"ex3_2\" # TODO name that allows you to identify your collection in the future\n",
    "db_persistent_directory = \"chroma/databases/\" + db_name # TODO adjust to your own chroma storage path\n",
    "# DATABASE CHROMA\n",
    "chroma_client = chromadb.Client(chromadb.config.Settings(\n",
    "            chroma_db_impl = \"duckdb+parquet\",\n",
    "            persist_directory = db_persistent_directory\n",
    "        ))\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=db_collection_name)\n",
    "print(\"Elements in loaded collection: \" + str(chroma_collection.count())) # should typically be empty\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "# any backbone from https://huggingface.co/timm\n",
    "backbone = \"vit_large_patch14_dinov2.lvd142m\" # TODO choose your timm backbone (ours: \"vit_small_patch14_dinov2.lvd142m\",\"vit_large_patch14_dinov2.lvd142m\",\"vit_base_patch16_clip_224.openai\",\"vit_large_patch14_clip_336.openai\")\n",
    "\n",
    "# MODEL\n",
    "model = timm.create_model(backbone, pretrained=True, num_classes=0).to(device)\n",
    "model = model.eval()\n",
    "\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transform = timm.data.create_transform(**data_config, is_training=False) # get the required transform for the given backbone\n",
    "\n",
    "if dataset == \"melanoma\":\n",
    "        print(\"Melanoma dataset\")\n",
    "        # load the csv file\n",
    "        csv_file_path = \"assets/datasets/ISIC2018/Task3-DiseaseClassification/isic_labels_unified_TRAIN.csv\"\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        print(df.head())\n",
    "\n",
    "        # Define and map the class label\n",
    "        class_labels = [\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]\n",
    "        removed_features_per_class = [1,5,10,20,30,40,50,60,70,80,90,100]\n",
    "        #{'MEL': 0, 'NV': 1, 'BCC': 2, 'AKIEC': 3, 'BKL': 4, 'DF': 5, 'VASC': 6}\n",
    "        # melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowen’s disease, benign keratosis, dermatofibroma, and vascular lesion\n",
    "        class_labels_map = {}\n",
    "        for indx, label in enumerate(class_labels):\n",
    "            class_labels_map[indx] = label\n",
    "\n",
    "\n",
    "        # GET TRAIN EMBEDDINGS (support set)\n",
    "        print('Calculate feature maps for support set embeddings...')\n",
    "        if pretrain:\n",
    "          print(\"Using pretrain\")\n",
    "          df_train = pd.read_csv(\"isic_labels_unified_TRAIN_embeddings.csv\")\n",
    "          df_train['embedding'] = df_train['embedding'].apply(ast.literal_eval)\n",
    "          df_train['metadata'] = df_train['metadata'].apply(ast.literal_eval)\n",
    "        else:\n",
    "          # Lets create an object from our custom dataset class\n",
    "          train_dataset = datasetISIC2018DiseaseClassification(csv_file_path, class_labels, transform, \"train\")\n",
    "\n",
    "          classes = train_dataset.classes\n",
    "          print(f\"classes: {classes} type:{type(classes)}\")\n",
    "          train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "          for batch_idx, (data, target) in enumerate(train_loader):\n",
    "              print(f\"Batch {batch_idx}: data shape = {data.shape}, target = {target}\")\n",
    "              if batch_idx == 0:  # Print only the first batch for verification\n",
    "                  break\n",
    "          df_train = make_embeddings(train_loader, model, classes, device)\n",
    "          df_train.to_csv(\"isic_labels_unified_TRAIN_embeddings.csv\", index=False)\n",
    "        print(df_train.head())\n",
    "\n",
    "        add_train_test_df_to_chroma_collection(chroma_collection,df_train)\n",
    "        chroma_client.persist() # data is only stored in Chroma DB when persist() is called\n",
    "\n",
    "        assert chroma_collection.count() == (len(df_train))\n",
    "        ## DO FOR RANDOM CHOICE\n",
    "        df_train_n = df_train\n",
    "        # add labels as own col to df\n",
    "        df_train_n['labels'] = [metadata['label'] for metadata in df_train_n.metadata] # had typo\n",
    "        df_train_n = df_train_n.drop(['embedding', 'document','metadata'], axis=1) # had typo\n",
    "\n",
    "        # Get unique labels\n",
    "        unique_labels = df_train_n['labels'].unique()\n",
    "\n",
    "        # Create a dictionary to store numpy arrays for each label\n",
    "        label_arrays = {}\n",
    "\n",
    "        # Iterate over unique labels and create numpy arrays\n",
    "        for label in unique_labels:\n",
    "            label_arrays[label] = np.array(df_train_n[df_train_n['labels'] == label]['id']) # had typo\n",
    "\n",
    "\n",
    "        ## SUPPORT SET HAS BEEN ADDED TO DB ##\n",
    "        # EVALUATE ACCURACY MELANOMA\n",
    "\n",
    "        if pretrain:\n",
    "          df_test = pd.read_csv(\"isic_labels_unified_TEST_embeddings.csv\")\n",
    "          df_test['embedding'] = df_test['embedding'].apply(ast.literal_eval)\n",
    "          df_test['metadata'] = df_test['metadata'].apply(ast.literal_eval)\n",
    "        else:\n",
    "          # load the csv file\n",
    "          csv_file_path = \"assets/datasets/ISIC2018/Task3-DiseaseClassification/isic_labels_unified_TEST.csv\"\n",
    "          df = pd.read_csv(csv_file_path)\n",
    "\n",
    "          # Define and map the class label\n",
    "          class_labels = [\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]\n",
    "          #{'MEL': 0, 'NV': 1, 'BCC': 2, 'AKIEC': 3, 'BKL': 4, 'DF': 5, 'VASC': 6}\n",
    "          # melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowen’s disease, benign keratosis, dermatofibroma, and vascular lesion\n",
    "          class_labels_map = {}\n",
    "          for indx, label in enumerate(class_labels):\n",
    "              class_labels_map[indx] = label\n",
    "\n",
    "          # Lets create an object from our custom dataset class\n",
    "          test_dataset = datasetISIC2018DiseaseClassification(csv_file_path, class_labels, transform, \"test\")\n",
    "\n",
    "          classes = test_dataset.classes\n",
    "          test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "          df_test = make_embeddings(test_loader, model, classes, device)\n",
    "          df_test.to_csv(\"isic_labels_unified_TEST_embeddings.csv\", index=False)\n",
    "        print(df_test.head())\n",
    "\n",
    "elif dataset == \"pneumonia\":\n",
    "        print(\"Pneumonia dataset\")\n",
    "        # DATASET # TODO choose your support dataset (ours: \"melanoma\", \"pneumonia\")\n",
    "        classes = ['NORMAL', 'PNEUMONIA'] if pretrain else train_dataset.classes\n",
    "        removed_features_per_class = [1,5,10,20,50,100,200,300,500,700,900]\n",
    "\n",
    "        print(f\"classes: {classes} type:{type(classes)}\")\n",
    "\n",
    "        # GET TRAIN EMBEDDINGS (support set)\n",
    "        print('Calculate feature maps for support set embeddings...')\n",
    "        if pretrain:\n",
    "            df_train = pd.read_csv(\"pneumonia_train_embeddings.csv\")\n",
    "            df_train['embedding'] = df_train['embedding'].apply(ast.literal_eval)\n",
    "            df_train['metadata'] = df_train['metadata'].apply(ast.literal_eval)\n",
    "\n",
    "        else:\n",
    "            train_dataset = torchvision.datasets.ImageFolder('assets/datasets/Pneumonia/train', transform=transform)\n",
    "            train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "            df_train = make_embeddings(train_loader, model, classes, device)\n",
    "            df_train.to_csv(\"pneumonia_train_embeddings.csv\", index=False)\n",
    "        print(df_train.head())\n",
    "        add_train_test_df_to_chroma_collection(chroma_collection,df_train)\n",
    "        chroma_client.persist() # data is only stored in Chroma DB when persist() is called\n",
    "\n",
    "        assert chroma_collection.count() == (len(df_train))\n",
    "\n",
    "\n",
    "        ## DO FOR RANDOM CHOICE\n",
    "        df_train_n = df_train\n",
    "        # add labels as own col to df\n",
    "        df_train_n['labels'] = [metadata['label'] for metadata in df_train_n.metadata] # had typo\n",
    "        df_train_n = df_train_n.drop(['embedding', 'document','metadata'], axis=1) # had typo\n",
    "\n",
    "        # Get unique labels\n",
    "        unique_labels = df_train_n['labels'].unique()\n",
    "\n",
    "        # Create a dictionary to store numpy arrays for each label\n",
    "        label_arrays = {}\n",
    "\n",
    "        # Iterate over unique labels and create numpy arrays\n",
    "        for label in unique_labels:\n",
    "            label_arrays[label] = np.array(df_train_n[df_train_n['labels'] == label]['id']) # had typo\n",
    "\n",
    "        ## SUPPORT SET HAS BEEN ADDED TO DB ##\n",
    "        # EVALUATE ACCURACY PNEUMONIA\n",
    "\n",
    "        classes = ['NORMAL', 'PNEUMONIA'] if pretrain else test_dataset.classes\n",
    "        if pretrain:\n",
    "          df_test = pd.read_csv(\"pneumonia_test_embeddings.csv\")\n",
    "          df_test['embedding'] = df_test['embedding'].apply(ast.literal_eval)\n",
    "          df_test['metadata'] = df_test['metadata'].apply(ast.literal_eval)\n",
    "        else:\n",
    "          # DATASET # TODO choose your test dataset (ours: \"melanoma\", \"pneumonia\")\n",
    "          test_dataset = torchvision.datasets.ImageFolder('assets/datasets/Pneumonia/test', transform=transform)\n",
    "          test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "          df_test = make_embeddings(test_loader, model, classes, device)\n",
    "          df_test.to_csv(\"pneumonia_test_embeddings.csv\", index=False)\n",
    "        \n",
    "        print(df_test.head())\n",
    "else:\n",
    "        print(\"Unknown dataset\")\n",
    "        exit()\n",
    "\n",
    "# Evaluate baseline acc with most significant elements\n",
    "acc_baseline, id_frequencies = eval_collection_against_df_count_most_significant(chroma_collection, df_test,classes)\n",
    "print(backbone + \":Classes \" + str(df_test.class_id.unique()) + \": acc_baseline= \" + str(acc_baseline))\n",
    "\n",
    "results = {}\n",
    "# Stepwise remove i random elements\n",
    "ids_random_elements_already_deleted = list()\n",
    "for i in removed_features_per_class:\n",
    "    # function was implemented wrong\n",
    "    ids_random_elements_already_deleted, last_acc = copy_collection_delete_random_n_and_eval(label_arrays, i, chroma_collection, chroma_client, db_collection_name, df_test, classes, ids_random_elements_already_deleted)\n",
    "    print(backbone + \":Delete \"+str(i)+\" random elements from each class. Acc= \" + str(last_acc))\n",
    "    results[str(i)] = last_acc\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarchutISIB24py3115",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
