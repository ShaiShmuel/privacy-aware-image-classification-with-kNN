{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb==0.3.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS AND IMPORTS\n",
    "import timm\n",
    "import chromadb\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "def make_embeddings(dataloader, model, classes, device):\n",
    "    data_source = \"\" # E.g. \"torch_cifar10\": string added as metadata to DB. Currently not further used, but might be helpful to determine data origin.\n",
    "    apped_str_to_ids = \"\" # string concatinated to each id string in DB. Helpful if multiple datasets should be added to the same DB\n",
    "    d = []\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        images = X\n",
    "        labels = y\n",
    "        images = images.to(device)\n",
    "\n",
    "        output = model.forward_features(images)\n",
    "        output = model.forward_head(output, pre_logits=True)\n",
    "\n",
    "        feature_maps = output\n",
    "\n",
    "\n",
    "        for id, feature_map in enumerate(feature_maps):\n",
    "            d.append(\n",
    "                {\n",
    "                    'document': str(\"X[\" + str(id) + \"]\"), # document is currently irrelevant. Might be used to add raw input data to DB\n",
    "                    'embedding': feature_map.flatten().tolist(), # this is important\n",
    "                    'metadata': {\"label\": classes[y[id]], \"data_source\": data_source}, # can add any metadata. \"label\" was used for ACC calculation on test data \n",
    "                    'id': str(\"id_b\" + str(batch) + \"_id\" + str(id) + apped_str_to_ids),\n",
    "                    'class_id': y[id].item()\n",
    "                }\n",
    "            )\n",
    "\n",
    "        ### ADD SOME GARBAGE COLLECTION\n",
    "        del X\n",
    "        del y\n",
    "        del images\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "def add_train_test_df_to_chroma_collection(chroma_collection, df):\n",
    "    chroma_collection.upsert(\n",
    "        documents=list(df['document']),\n",
    "        embeddings=list(df['embedding']),\n",
    "        metadatas=list(df['metadata']),\n",
    "        ids=list(df['id'])\n",
    "    )\n",
    "    print(\"Elements in DB after add:\", chroma_collection.count())  # returns the number of items in the collection\n",
    "    #print(collection.peek()) # returns a list of the first 10 items in the collection\n",
    "\n",
    "# PHASE 3 (inference)\n",
    "def eval_collection_against_df(collection_loaded, df_to_test, classes, k = 10, distance_weighted = False):\n",
    "        total_correct = 0\n",
    "        total_instances = 0\n",
    "\n",
    "        for sample in df_to_test.itertuples():\n",
    "            print(\"Testing \" + str(total_instances) + \" out of \" + str(len(df_to_test.index)), end=\"\\r\")\n",
    "            result = collection_loaded.query(\n",
    "                query_embeddings=list(sample.embedding),\n",
    "                n_results = k,\n",
    "                include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "            )\n",
    "\n",
    "            if distance_weighted == False:\n",
    "                classification = knn_majority_vote(result, k)\n",
    "            else:\n",
    "                classification = knn_distance_weighted(result, k)\n",
    "\n",
    "            if (classification == classes[sample.class_id]):\n",
    "                total_correct+=1\n",
    "                #print(\"Correct classification\")\n",
    "            #else:\n",
    "                #print(\"Missclassification: Real: \" + classes[sample.class_id] + \"; Classification:\" + classification)\n",
    "                #print(display_distances_with_labels(result))\n",
    "            total_instances+=1\n",
    "\n",
    "        acc = round(total_correct/total_instances, 3)\n",
    "        #print(\"Total test instances: \" + str(total_instances))\n",
    "        #print(\"Total correct of test instances: \" + str(total_correct))\n",
    "        #print(\"Accuracy: \" +  str(acc))\n",
    "\n",
    "        return acc\n",
    "\n",
    "def knn_majority_vote(data, k):\n",
    "    distances = data['distances'][0]\n",
    "    labels = [metadata['label'] for metadata in data['metadatas'][0]]\n",
    "\n",
    "    # Get the k nearest neighbors based on distances\n",
    "    nearest_neighbors = sorted(range(len(distances)), key=lambda x: distances[x])[:k]\n",
    "    \n",
    "    # Extract the corresponding labels for the nearest neighbors\n",
    "    nearest_labels = [labels[i] for i in nearest_neighbors]\n",
    "\n",
    "    # Perform majority vote on the labelss\n",
    "    vote_counts = Counter(nearest_labels)\n",
    "    \n",
    "    # Get the label with the maximum vote count\n",
    "    majority_label = vote_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return majority_label\n",
    "\n",
    "def knn_distance_weighted(data, k):\n",
    "    distances = data['distances'][0]\n",
    "    labels = [metadata['label'] for metadata in data['metadatas'][0]]\n",
    "\n",
    "    # Get the k nearest neighbors based on distances\n",
    "    nearest_neighbors = sorted(range(len(distances)), key=lambda x: distances[x])[:k]\n",
    "\n",
    "    # Create a dictionary to store label weights\n",
    "    label_weights = {}\n",
    "\n",
    "    # Calculate distance-weighted votes for each label\n",
    "    for i in nearest_neighbors:\n",
    "        if (distances[i] != 0):\n",
    "            weight = 1.0 / distances[i]\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        label = labels[i]\n",
    "        if label in label_weights:\n",
    "            label_weights[label] += weight\n",
    "        else:\n",
    "            label_weights[label] = weight\n",
    "\n",
    "    # Find the label with the maximum weighted vote\n",
    "    majority_label = max(label_weights, key=label_weights.get)\n",
    "\n",
    "    return majority_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP PARAMETERS\n",
    "device = \"cuda:0\" # TODO choose your ML computing device\n",
    "db_name = \"privacy-aware-image-classification-with-kNN\" # TODO name that allows you to identify db in the future\n",
    "db_collection_name = db_name + \"_\" + \"ex1_STL10\" # TODO name that allows you to identify your collection in the future\n",
    "db_persistent_directory = \"chroma/databases/\" + db_name # TODO adjust to your own chroma storage path\n",
    "# DATABASE CHROMA\n",
    "chroma_client = chromadb.Client(chromadb.config.Settings(\n",
    "            chroma_db_impl = \"duckdb+parquet\",\n",
    "            persist_directory = db_persistent_directory\n",
    "        ))\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=db_collection_name)\n",
    "print(\"Elements in loaded collection: \" + str(chroma_collection.count())) # should typically be empty\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "# any backbone from https://huggingface.co/timm\n",
    "backbone = \"vit_small_patch14_dinov2.lvd142m\" # TODO choose your timm backbone (ours: \"vit_small_patch14_dinov2.lvd142m\",\"vit_large_patch14_dinov2.lvd142m\",\"vit_base_patch16_clip_224.openai\",\"vit_large_patch14_clip_336.openai\")\n",
    "\n",
    "# MODEL\n",
    "model = timm.create_model(backbone, pretrained=True, num_classes=0).to(device)\n",
    "model = model.eval()\n",
    "\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transform = timm.data.create_transform(**data_config, is_training=False) # get the required transform for the given backbone\n",
    "\n",
    "# DATASET # TODO choose your support dataset (ours: \"STL10\", \"CIFAR-10\", \"CIFAR-100\")\n",
    "train_dataset = torchvision.datasets.STL10(root='./data', split=\"train\", download=True, transform=transform)\n",
    "classes = train_dataset.classes\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# GET TRAIN EMBEDDINGS (support set)\n",
    "print('Calculate feature maps for support set embeddings...')\n",
    "df_train = make_embeddings(train_loader, model, classes, device)\n",
    "add_train_test_df_to_chroma_collection(chroma_collection,df_train)\n",
    "chroma_client.persist() # data is only stored in Chroma DB when persist() is called\n",
    "\n",
    "assert chroma_collection.count() == (len(df_train))\n",
    "\n",
    "## SUPPORT SET HAS BEEN ADDED TO DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE ACCURACY\n",
    "# DATASET # TODO choose your test dataset (ours: \"STL10\", \"CIFAR-10\", \"CIFAR-100\")\n",
    "test_dataset = torchvision.datasets.STL10(root='./data', split=\"test\", download=True, transform=transform)\n",
    "classes = test_dataset.classes\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "df_test = make_embeddings(test_loader, model, classes, device)\n",
    "\n",
    "acc = eval_collection_against_df(chroma_collection, df_test, classes, k = 10)\n",
    "\n",
    "print(backbone + \":Classes \" + str(df_test.class_id.unique()) + \": acc= \" + str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarchutISIB24py3115",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
